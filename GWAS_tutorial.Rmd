---
title: "GWAS tutorial"
author: "Eryn McFarlane"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Learning outcomes of GWAS tutorial
1) Understand what a GWAS is, and how to run it in gemma
2) Think about specific predictions for GWAS, and what we really expect to see for most of them
3) Think critically about when GWAS are robust, and when they should be taken skeptically, and why they're interesting to do anyway


### Simulations
The first section here is a simulation of 10000 SNPs and 2000 individuals. While there are definitely some much, much larger GWAS than this, even in wild systems, many studies are quite a bit smaller than this. Another thing to note is that I haven't simulated any chromosomes or LD structure. So these SNPs are all independent. That isn't really realistic for most GWAS. Finally, when I've simulated the traits (each a categorical trait, a quantitative trait with few causal loci and a quantitative trait with many causal loci), I have simulated causal loci. This is not expected to be the case for most GWAS - we're hoping to hit SNPs that are in LD with the causal regions, but rarely can we expect to actually genotype causal SNPs. This is why, in general, the linkage situation is so important. 
Specifically, I've used drawn from a binomial distribution (rbinom on line 39, which has a probability drawn from a beta. The beta is symmetrical with a high parameter (alpha = beta = 5) which will give lots of polymorphisms for our SNPs.)


Tl;dr I've hard coded the simulations here, and I'm not going to talk a lot about how I've done them, I wanted to give everyone the code to be able to do this yourself. But the first chunk of code can be run without any additional input.


```{r simulations}
### This is from Alex Buerkle's Population Genetics lab notes
###simulate some data
set.seed(42)
nloci<-10000  ## we will have data from nloci
nind<-2000  ## we will have data from nind that were sampled from the true population
sim.theta<-5  # high parameter means high polymorphism, must be positive and > zero, can be thought of as analygous to nucleotide polymorphism

## simulate allele frequencies at nloci number of loci, by random draws from a beta
sim.p<-rbeta(nloci, sim.theta, sim.theta)
hist(sim.p, breaks=seq(0,1,0.1))

# simulate genotypes in the sample
sim.x <- matrix(rbinom(nloci*nind, 2, prob=sim.p), nrow=nind, ncol=nloci) ### this gives us individuals in rows and snps in columns
str(sim.x)
### this is like the .ped set up, but not entirely. 

###some things to note - these are assumed to be independent. This isn't realistic, there would certainly be LD
### I haven't simulated chromosomes, just allele frequencies

### one more thing to simulate is the phenotypes, and the number of markers that they're based one.

### let's do one that's categorical and based on one SNP, like the horn traits in soay sheep

y_cat<-5+0.7*sim.x[,1]+0.7*sim.x[,100]+0.7*sim.x[,1000]+0.7*sim.x[,1500]+rnorm(2000, 0, 1) ###large effect sizes, not a lot of error

summary(lm(y_cat~sim.x[,1]+sim.x[,100]+sim.x[,1000]+sim.x[,1500]))

summary(lm(y_cat~sim.x[,2]+sim.x[,102]+sim.x[,1002]+sim.x[,1502]))

y_quant<-0.1*sim.x[,10]+0.1*sim.x[,20]+0.1*sim.x[,30]+0.1*sim.x[,40]-0.1*sim.x[,50]-0.1*sim.x[,60]-0.1*sim.x[,70]+0.1*sim.x[,80]+0.1*sim.x[,90]+0.1*sim.x[,100]+rnorm(2000, 0, 5) ###more snps, larger error, smaller effect sizes

hist(y_quant)

y_polygenic<-0.0001*sim.x[,11]+ ###100 snps, larger error, quite small effect sizes
   0.0001*sim.x[,21]+ 
  0.0001*sim.x[,31]+ 
  0.0001*sim.x[,41]+ 
  0.0001*sim.x[,51]+ 
  0.0001*sim.x[,61]+ 
  0.0001*sim.x[,71]+ 
  0.0001*sim.x[,81]+ 
  0.0001*sim.x[,91]+ 
  0.0001*sim.x[,101]+ 
  0.0001*sim.x[,111]+ 
  0.0001*sim.x[,121]+ 
  0.0001*sim.x[,131]+ 
  0.0001*sim.x[,141]+ 
  0.0001*sim.x[,151]+ 
  0.0001*sim.x[,161]+ 
  0.0001*sim.x[,171]+ 
  0.0001*sim.x[,181]+ 
  0.0001*sim.x[,191]+ 
  0.0001*sim.x[,201]+ 
  0.0001*sim.x[,211]+ 
  0.0001*sim.x[,221]+ 
  0.0001*sim.x[,231]+ 
  0.0001*sim.x[,241]+ 
  0.0001*sim.x[,251]+ 
  0.0001*sim.x[,261]+ 
  0.0001*sim.x[,271]+ 
  0.0001*sim.x[,281]+ 
  0.0001*sim.x[,291]+ 
  0.0001*sim.x[,301]+ 
  0.0001*sim.x[,311]+ 
  0.0001*sim.x[,321]+ 
  0.0001*sim.x[,331]+ 
  0.0001*sim.x[,341]+ 
  0.0001*sim.x[,351]+ 
  0.0001*sim.x[,361]+ 
  0.0001*sim.x[,371]+ 
  0.0001*sim.x[,381]+ 
  0.0001*sim.x[,391]+ 
  0.0001*sim.x[,401]+ 
  0.0001*sim.x[,411]+ 
  0.0001*sim.x[,421]+ 
  0.0001*sim.x[,431]+ 
  0.0001*sim.x[,441]+ 
  0.0001*sim.x[,451]+ 
  0.0001*sim.x[,461]+ 
  0.0001*sim.x[,471]+ 
  0.0001*sim.x[,481]+ 
  0.0001*sim.x[,491]+ 
  0.0001*sim.x[,501]+ 
  0.0001*sim.x[,511]+ 
  0.0001*sim.x[,521]+ 
  0.0001*sim.x[,531]+ 
  0.0001*sim.x[,541]+ 
  0.0001*sim.x[,551]+ 
  0.0001*sim.x[,561]+ 
  0.0001*sim.x[,571]+ 
  0.0001*sim.x[,581]+ 
  0.0001*sim.x[,591]+ 
  0.0001*sim.x[,601]+ 
  0.0001*sim.x[,611]+ 
  0.0001*sim.x[,621]+ 
  0.0001*sim.x[,631]+ 
  0.0001*sim.x[,641]+ 
  0.0001*sim.x[,651]+ 
  0.0001*sim.x[,661]+ 
  0.0001*sim.x[,671]+ 
  0.0001*sim.x[,681]+ 
  0.0001*sim.x[,691]+ 
  0.0001*sim.x[,701]+ 
  0.0001*sim.x[,711]+ 
  0.0001*sim.x[,721]+ 
  0.0001*sim.x[,731]+ 
  0.0001*sim.x[,741]+ 
  0.0001*sim.x[,751]+ 
  0.0001*sim.x[,761]+ 
  0.0001*sim.x[,771]+ 
  0.0001*sim.x[,781]+ 
  0.0001*sim.x[,791]+ 
  0.0001*sim.x[,801]+ 
  0.0001*sim.x[,811]+ 
  0.0001*sim.x[,821]+ 
  0.0001*sim.x[,831]+ 
  0.0001*sim.x[,841]+ 
  0.0001*sim.x[,851]+ 
  0.0001*sim.x[,861]+ 
  0.0001*sim.x[,871]+ 
  0.0001*sim.x[,881]+ 
  0.0001*sim.x[,891]+ 
  0.0001*sim.x[,901]+ 
  0.0001*sim.x[,911]+ 
  0.0001*sim.x[,921]+ 
  0.0001*sim.x[,931]+ 
  0.0001*sim.x[,941]+ 
  0.0001*sim.x[,951]+ 
  0.0001*sim.x[,961]+ 
  0.0001*sim.x[,971]+ 
  0.0001*sim.x[,981]+ 
  0.0001*sim.x[,991]+ 
  0.0001*sim.x[,1001]+rnorm(2000, 0, 5)


### need to output something that looks like a bimbam datafiles

```

### Choose your own adventure 

So this is where I think most people will skip to. Please replace the set.seed with your own favourite number. This is down sampling us to 1000 individuals and only 5000 (unlinked remember) loci. From here, we will print out the data in the appropriate form to be able to run a BSLMM. 

I kind of want everyone to try all of this themselves, and, please put some of the output into a googlesheet:
https://docs.google.com/spreadsheets/d/1W0ovcjzTr96GOBQDDSrGz6_xmCed1XHGtwjIqmAUEm0/edit?usp=sharing


```{r choose your own adventure}
set.seed(42) ### put in your favourite number here!

test_individuals<-sample(1:2000, 1000, replace=FALSE)
test_snps<-sample(1:10000, 5000, replace=FALSE)

write.table(y_cat[test_individuals], row.names=FALSE, col.names=FALSE, file="y_cat.txt")
write.table(y_quant[test_individuals], row.names=FALSE, col.names=FALSE, file="y_quant.txt")
write.table(y_polygenic[test_individuals], row.names=FALSE, col.names=FALSE, file="y_polygenic.txt")

SNPs<-t(sim.x[test_individuals, test_snps]) ###this is to use the sub-sampled data only. the transposing of the matrix is what helps to put it into bimbam form. I've gotten stuck on this before!
str(SNPs)

SNPs_bimbam<- cbind(1:length(rowMeans(SNPs)), rowMeans(SNPs), rowMeans(SNPs), SNPs) ### this is what makes it into a bimbam file we need

write.table(SNPs_bimbam, row.names = FALSE, col.names = FALSE, file="SNPs.txt")

```

### gemma for the cateogorical trait

Before we jump into gemma, I want to just quickly walk everyone through what it does. 
The first step is to calculate a genomic relatedness matrix/kinship matrix. 
The second step is to do the actual GWAS, while controlling for the relationship matrix
Do note, that although I haven't either simulated or modelled this here, it is possible to put additional fixed effects in the BSLMM, and if that's something you're interested in, I can help you do it. 
It's also possible to use BSLMM to do prediction of unmeasured individuals, and that's something else I can help with, but that we haven't done here. 

I will also say that both the gemma manual, and the gemma google group are really good. There's a lot of excellent support to use this program, which is part of why I chose it for today.

- g genotypes input (as a bimbam)
- p phenotype file
-gk specifies the type of relatedness matrix generate (1 is default)
-o output

-k input of relatedness matrix
-bslmm 1 (run linear bslmm), 2 (run ridge regression), 3 probit BSLMM
-s sampling steps
-w burnin steps

```{r gemma categorical trait}

### I didn't change a lot of the names of things, so this will over-write other sections
library(coda)
library(ggplot2)

### categorical trait
  ### actually run gemma first
  system("~/gemma -g SNPs.txt -p y_cat.txt -gk 1 -o relmatrix", wait=TRUE) ### gets the relationship matrix
  system("~/gemma -g SNPs.txt -p y_cat.txt -k output/relmatrix.cXX.txt -bslmm 1 -s 50000 -w 5000 -o y_cat", wait=TRUE)
  
  hyp.params <- read.table("output/y_cat.hyp.txt", header=TRUE)
  converged <- ifelse(heidel.diag(hyp.params$pve)[1,4]==1, "converged", "not_converged") ### if not converged, run longer!
  converged
  pve<-c("PVE", mean(hyp.params$pve),quantile(hyp.params$pve, probs=c(0.5,0.025,0.975))) ## proportion of 
  ## pge -> proportion of genetic variance explained by major effect loci
  pge <- c("PGE",mean(hyp.params$pge),quantile(hyp.params$pge, probs=c(0.5,0.025,0.975)))
  ## n.gamma -> number of variants with major effect
  n.gamma <- c("n.gamma",mean(hyp.params$n_gamma),quantile(hyp.params$n_gamma, probs=c(0.5,0.025,0.975))) ### gives the estimate of SNPs that have a sparse (large enough to measure) effect on the trait

  params <- read.table("output/y_cat.param.txt",header=T,sep="\t")
  params$SNPs<-paste("SNP", test_snps) ###giving the SNPs the names from the original simulations 
  params["eff"]<-abs(params$beta*params$gamma) # add sparse effect size (= beta * gamma) to data frame

params.effects<-params[params$eff>0,]

# show number of variants with measurable effect
nrow(params.effects)
params.effects.sort<-params.effects[order(-params.effects$eff),]
# show top 10 variants with highest effect
head(params.effects.sort, 11) 
top01<-params.effects.sort[params.effects.sort$eff>quantile(params.effects.sort$eff,0.999),]
top01
# sort variants by descending PIP
params.pipsort<-params[order(-params$gamma),]

# Show top 10 variants with highest PIP
head(params.pipsort,13)
plot<-ggplot(params, aes(rs, gamma))+geom_jitter(size=3)+theme_bw()+geom_hline(yintercept=0.10, linetype='dashed', colour="grey")+theme(legend.position='none')+xlab("SNPs")+ylab("PIP")+theme(axis.title=element_text(size=14))+ylim(0,1)

### do something so that this goes all the way up to PIP 1 - as of right now, none of these are over the threshold!

plot+geom_text(aes(label=ifelse(gamma>0.15, SNPs, "")), hjust="inward",vjust="inward", size=5, nudge_x = 0.5)

```

## gemma for the Quantitative Trait

```{r gemma quantitative trait, echo=FALSE}

library(coda)
library(ggplot2)

### quantitative trait
  ### actually run gemma first
  system("~/gemma -g SNPs.txt -p y_quant.txt -gk 1 -o relmatrix", wait=TRUE) ### gets the relationship matrix
  system("~/gemma -g SNPs.txt -p y_quant.txt -k output/relmatrix.cXX.txt -bslmm 1 -s 50000 -w 5000 -o y_quant", wait=TRUE)
  
  hyp.params <- read.table("output/y_quant.hyp.txt", header=TRUE)
  converged <- ifelse(heidel.diag(hyp.params$pve)[1,4]==1, "converged", "not_converged") ### if not converged, run longer!
  converged
  pve<-c("PVE", mean(hyp.params$pve),quantile(hyp.params$pve, probs=c(0.5,0.025,0.975))) ## proportion of 
  ## pge -> proportion of genetic variance explained by major effect loci
  pge <- c("PGE",mean(hyp.params$pge),quantile(hyp.params$pge, probs=c(0.5,0.025,0.975)))
  ## n.gamma -> number of variants with major effect
  n.gamma <- c("n.gamma",mean(hyp.params$n_gamma),quantile(hyp.params$n_gamma, probs=c(0.5,0.025,0.975))) ### gives the estimate of SNPs that have a sparse (large enough to measure) effect on the trait

  params <- read.table("output/y_quant.param.txt",header=T,sep="\t")
  params$SNPs<-paste("SNP", test_snps)###giving the SNPs the names from the original simulations 
  params["eff"]<-abs(params$beta*params$gamma) # add sparse effect size (= beta * gamma) to data frame

params.effects<-params[params$eff>0,]

# show number of variants with measurable effect
nrow(params.effects)
params.effects.sort<-params.effects[order(-params.effects$eff),]
# show top 10 variants with highest effect
head(params.effects.sort, 11) 
top01<-params.effects.sort[params.effects.sort$eff>quantile(params.effects.sort$eff,0.999),]

# sort variants by descending PIP
params.pipsort<-params[order(-params$gamma),]

# Show top 10 variants with highest PIP
head(params.pipsort,13)
plot<-ggplot(params, aes(as.factor(SNPs), gamma))+geom_jitter(size=3)+theme_bw()+geom_hline(yintercept=0.10, linetype='dashed', colour="grey")+theme(legend.position='none')+xlab("SNPs")+ylab("PIP")+theme(axis.title=element_text(size=14))+ylim(0,1)

### do something so that this goes all the way up to PIP 1 - as of right now, none of these are over the threshold!

plot+geom_text(aes(label=ifelse(gamma>0.15, SNPs, "")), hjust="inward",vjust="inward", size=5, nudge_x = 0.5)



```

## gemma for the Polygenic Trait

```{r gemma polygenic trait}

### I didn't change a lot of the names of things, so this will over-write other sections
library(coda)
library(ggplot2)

### categorical trait
  ### actually run gemma first
  system("~/gemma.macosx -g SNPs.txt -p y_polygenic.txt -gk 1 -o relmatrix", wait=TRUE) ### gets the relationship matrix
  system("~/gemma.macosx -g SNPs.txt -p y_polygenic.txt -k output/relmatrix.cXX.txt -bslmm 1 -s 50000 -w 5000 -o y_polygenic", wait=TRUE)
  
  hyp.params <- read.table("output/y_polygenic.hyp.txt", header=TRUE)
  converged <- ifelse(heidel.diag(hyp.params$pve)[1,4]==1, "converged", "not_converged") ### if not converged, run longer!
  converged
  pve<-c("PVE", mean(hyp.params$pve),quantile(hyp.params$pve, probs=c(0.5,0.025,0.975))) ## proportion of 
  ## pge -> proportion of genetic variance explained by major effect loci
  pge <- c("PGE",mean(hyp.params$pge),quantile(hyp.params$pge, probs=c(0.5,0.025,0.975)))
  ## n.gamma -> number of variants with major effect
  n.gamma <- c("n.gamma",mean(hyp.params$n_gamma),quantile(hyp.params$n_gamma, probs=c(0.5,0.025,0.975))) ### gives the estimate of SNPs that have a sparse (large enough to measure) effect on the trait

  params <- read.table("output/y_polygenic.param.txt",header=T,sep="\t")
  params$SNPs<-paste("SNP", test_snps) ###giving the SNPs the names from the original simulations 
  params["eff"]<-abs(params$beta*params$gamma) # add sparse effect size (= beta * gamma) to data frame

params.effects<-params[params$eff>0,]

# show number of variants with measurable effect
nrow(params.effects)
params.effects.sort<-params.effects[order(-params.effects$eff),]
# show top 10 variants with highest effect
head(params.effects.sort, 11) 
top01<-params.effects.sort[params.effects.sort$eff>quantile(params.effects.sort$eff,0.999),]
top01
# sort variants by descending PIP
params.pipsort<-params[order(-params$gamma),]

# Show top 10 variants with highest PIP
head(params.pipsort,13)
plot<-ggplot(params, aes(SNPs, gamma))+geom_jitter(size=3)+theme_bw()+geom_hline(yintercept=0.10, linetype='dashed', colour="grey")+theme(legend.position='none')+xlab("SNPs")+ylab("PIP")+theme(axis.title=element_text(size=14))+ylim(0,1)

### do something so that this goes all the way up to PIP 1 - as of right now, none of these are over the threshold!

plot+geom_text(aes(label=ifelse(gamma>0.15, SNPs, "")), hjust="inward",vjust="inward", size=5, nudge_x = 0.5)

```

### Questions to think about:
- what is the typical sample size for your population? Is it possible to get repeated measures?
- how related are the individuals that could be sampled? Does this high relatedness help or hurt the changes of finding markers of large effect?
- What sort of trait are you interested in? Do you expect it to have a polygenic genomic architecture, and how could this change your sampling protocol?
- What does immigration into and emigration from your population look like? How does a closed population affect GWAS predictions?

### Google sheet again:

https://docs.google.com/spreadsheets/d/1W0ovcjzTr96GOBQDDSrGz6_xmCed1XHGtwjIqmAUEm0/edit?usp=sharing

### Other things that can be done, if there's interest, or if this workshop ends up being way too short
- change the set.seed on line 175. Do this a couple of times (as we've been doing as a group). How does this change your results for each of the different genetic architectures?
- vary the theta parameters on line 32, or make an alpha and beta theta to make the distribution asymmetrical. How does changing the underlying distribution, and thus the potential for polymorphism, change the recovery of true positive GWAS hits. What does this mean in, for example, inbreed populations?
- vary the number of SNPs or the number of individuals retained line 177 and 178.
- ofc, run this with your own biallelic data. What happens when you change the QC beforehand?
- use the 'predict' function in GEMMA to see how well the data that you've selected can predict the phenotype of the data that have been left out. To do this, instead of eliminating individuals on line 177, set their phenotypes to 'NA'; the second gemma command looks something like gemma -g SNPs.txt -p y_polygenic.txt -epm output/sims.param.txt -emu output/sims.log.txt -ebv output/sims.bv.txt -k output/relmatrix.cXX.txt -predict 1 -o sims.pheno <- do note, I haven't tested this line, so it might not perfectly match the above tutorial.

### Notes:
 - https://visoca.github.io/popgenomworkshop-gwas_gemma/ is a longer, more in depth GWAS tutorial using GEMMA that I used when I first started with GEMMA, so there are lots of simularities between this tutorial and that one. I recommend that one as well, especially when working with admixed data
 - As noted above, my simulations are based off those used by Alex Buerkle (formerly at UWyo) throughout his population genetics classes, so if you've ever taken a class from him (or one of his other disciples) they might seem very familiar. 